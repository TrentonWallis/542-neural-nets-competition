{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Desampling y for final predictions.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"J-_wQPlIxomJ"},"outputs":[],"source":["\n","# necessary imports\n","import numpy as np\n","import csv\n","import os\n","from google.colab import drive \n","import matplotlib.pyplot as plt\n","import random # Added by Caleb\n","import math # Added by Caleb\n","import os\n","import torch\n","import pandas as pd\n","from torchvision.io import read_image\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n"]},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/drive/') ## mount to drive. This will ask for permission to access your Google drive each time"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3c_WuMrOzN7T","executionInfo":{"status":"ok","timestamp":1648582656439,"user_tz":240,"elapsed":38808,"user":{"displayName":"Trenton Wallis","userId":"12490720454166063115"}},"outputId":"f1ff4da0-5ae2-4ff8-8823-145424388b97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["data_folder_path = \"/content/drive/Shareddrives/Neural Nets/Competition/ECE542_sp2022_Project_TerrainRecognition/\" # path into Lobton's directory \n","type_of_data = \"TrainingData\" # Read in the type of data you want. Options are either:  'TrainingData' or 'TestData'\n","list_of_files = os.listdir(data_folder_path + type_of_data) # List everything in the directory at place 2022_Project_TerrainRecognition/TrainingData or /TestData (from line above)\n","list_of_files.sort() # Sort the list of files\n","print(list_of_files) # Print out the list of files\n","# need to load all this data in for augmentation (only the x data, but need to match what y data it connects with)\n","# we need to figure out what data we want to use too. "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJJ9EhzAzOQU","executionInfo":{"status":"ok","timestamp":1648582659272,"user_tz":240,"elapsed":2838,"user":{"displayName":"Trenton Wallis","userId":"12490720454166063115"}},"outputId":"e7302352-71fc-4987-9f74-5b5c6dbf4463"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['subject_001_01__x.csv', 'subject_001_01__x_time.csv', 'subject_001_01__y.csv', 'subject_001_01__y_time.csv', 'subject_001_02__x.csv', 'subject_001_02__x_time.csv', 'subject_001_02__y.csv', 'subject_001_02__y_time.csv', 'subject_001_03__x.csv', 'subject_001_03__x_time.csv', 'subject_001_03__y.csv', 'subject_001_03__y_time.csv', 'subject_001_04__x.csv', 'subject_001_04__x_time.csv', 'subject_001_04__y.csv', 'subject_001_04__y_time.csv', 'subject_001_05__x.csv', 'subject_001_05__x_time.csv', 'subject_001_05__y.csv', 'subject_001_05__y_time.csv', 'subject_001_06__x.csv', 'subject_001_06__x_time.csv', 'subject_001_06__y.csv', 'subject_001_06__y_time.csv', 'subject_001_07__x.csv', 'subject_001_07__x_time.csv', 'subject_001_07__y.csv', 'subject_001_07__y_time.csv', 'subject_001_08__x.csv', 'subject_001_08__x_time.csv', 'subject_001_08__y.csv', 'subject_001_08__y_time.csv', 'subject_002_01__x.csv', 'subject_002_01__x_time.csv', 'subject_002_01__y.csv', 'subject_002_01__y_time.csv', 'subject_002_02__x.csv', 'subject_002_02__x_time.csv', 'subject_002_02__y.csv', 'subject_002_02__y_time.csv', 'subject_002_03__x.csv', 'subject_002_03__x_time.csv', 'subject_002_03__y.csv', 'subject_002_03__y_time.csv', 'subject_002_04__x.csv', 'subject_002_04__x_time.csv', 'subject_002_04__y.csv', 'subject_002_04__y_time.csv', 'subject_002_05__x.csv', 'subject_002_05__x_time.csv', 'subject_002_05__y.csv', 'subject_002_05__y_time.csv', 'subject_003_01__x.csv', 'subject_003_01__x_time.csv', 'subject_003_01__y.csv', 'subject_003_01__y_time.csv', 'subject_003_02__x.csv', 'subject_003_02__x_time.csv', 'subject_003_02__y.csv', 'subject_003_02__y_time.csv', 'subject_003_03__x.csv', 'subject_003_03__x_time.csv', 'subject_003_03__y.csv', 'subject_003_03__y_time.csv', 'subject_004_01__x.csv', 'subject_004_01__x_time.csv', 'subject_004_01__y.csv', 'subject_004_01__y_time.csv', 'subject_004_02__x.csv', 'subject_004_02__x_time.csv', 'subject_004_02__y.csv', 'subject_004_02__y_time.csv', 'subject_005_01__x.csv', 'subject_005_01__x_time.csv', 'subject_005_01__y.csv', 'subject_005_01__y_time.csv', 'subject_005_02__x.csv', 'subject_005_02__x_time.csv', 'subject_005_02__y.csv', 'subject_005_02__y_time.csv', 'subject_005_03__x.csv', 'subject_005_03__x_time.csv', 'subject_005_03__y.csv', 'subject_005_03__y_time.csv', 'subject_006_01__x.csv', 'subject_006_01__x_time.csv', 'subject_006_01__y.csv', 'subject_006_01__y_time.csv', 'subject_006_02__x.csv', 'subject_006_02__x_time.csv', 'subject_006_02__y.csv', 'subject_006_02__y_time.csv', 'subject_006_03__x.csv', 'subject_006_03__x_time.csv', 'subject_006_03__y.csv', 'subject_006_03__y_time.csv', 'subject_007_01__x.csv', 'subject_007_01__x_time.csv', 'subject_007_01__y.csv', 'subject_007_01__y_time.csv', 'subject_007_02__x.csv', 'subject_007_02__x_time.csv', 'subject_007_02__y.csv', 'subject_007_02__y_time.csv', 'subject_007_03__x.csv', 'subject_007_03__x_time.csv', 'subject_007_03__y.csv', 'subject_007_03__y_time.csv', 'subject_007_04__x.csv', 'subject_007_04__x_time.csv', 'subject_007_04__y.csv', 'subject_007_04__y_time.csv', 'subject_008_01__x.csv', 'subject_008_01__x_time.csv', 'subject_008_01__y.csv', 'subject_008_01__y_time.csv']\n"]}]},{"cell_type":"code","source":["class CustomImageDataset(Dataset):\n","    def __init__(self, x_data, y_data, img_dir=None, transform=None, target_transform=None):\n","        self.img_labels = y_data\n","        self.img_dir = img_dir\n","        self.x_data = x_data\n","        self.y_data = y_data\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        image = self.x_data[idx]\n","        label = self.y_data[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label\n"],"metadata":{"id":"uZjKtqu5Vv-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Code block created by Caleb\n","# Goal: Session class that holds subject and session number, and also returns the data for that session\n","\n","# This class can return the name of the file that holds the requested data\n","    # Example of how to use:\n","    # newSession = Session(4, 5)\n","    # newStr = newSession.xTimeName()\n","    # print(newStr)\n","    # # Prints out \"subject_004_05__x.csv\"\n","\n","class Session:\n","  def __init__(self, subject_number, session_number):\n","    self.subject_number = int(subject_number)  # Initialize subject number\n","    self.session_number = int(session_number)  # Initialize session number\n","\n","  # Each of the following member functions return the name of the specified file for that subject and session number of the Session:\n","\n","  def xTimeName(self):\n","    return \"subject_\" + f\"{self.subject_number:03d}\" + \"_\" + f\"{self.session_number:02d}\" + \"__x_time.csv\"\n","\n","  def yTimeName(self):\n","    return \"subject_\" + f\"{self.subject_number:03d}\" + \"_\" + f\"{self.session_number:02d}\" + \"__y_time.csv\"\n","\n","  def xDataName(self):\n","    return \"subject_\" + f\"{self.subject_number:03d}\" + \"_\" + f\"{self.session_number:02d}\" + \"__x.csv\"\n","\n","  def yDataName(self):\n","    return \"subject_\" + f\"{self.subject_number:03d}\" + \"_\" + f\"{self.session_number:02d}\" + \"__y.csv\"\n","\n","\n","  # This function input is the Session object that contians the subject and session numbers\n","  def getXDataFromFile(self):\n","    x_data_path = data_folder_path + \"TrainingData/\" + self.xDataName() # Get the path to the x_data file \n","    x_data = np.genfromtxt(x_data_path, delimiter=',')  # Read the data in from the text file\n","    return x_data  # Return the data array\n","    # This function returns the array of all the six x values\n","\n","  def getXTimeFromFile(self):\n","    x_time_path = data_folder_path + \"TrainingData/\" + self.xTimeName() # Get the path to the x_data file \n","    x_time = np.genfromtxt(x_time_path, delimiter=',')  # Read the data in from the text file\n","    return x_time  # Return the data array\n","    # This function returns the array of just the x time values\n","\n","  def getYTimeFromFile(self):\n","    y_time_path = data_folder_path + \"TrainingData/\" + self.yTimeName() # Get the path to the x_data file \n","    y_time = np.genfromtxt(y_time_path, delimiter=',')  # Read the data in from the text file\n","    return y_time  # Return the data array\n","    # This function returns the array of just the y time values\n","\n","\n"],"metadata":{"id":"9rupYvJ8zg7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Code block created by Caleb:\n","# Goal is to create list of each session, with a unique session id (session id made up of subject number and session number)\n","\n","list_of_sessions = []\n","\n","for file_name in list_of_files:\n","  # Extract subject and session number from file_name:\n","  new_subject_number = int(file_name[8:11]) # Since all provided subject and session numbers are single digit, we can \n","  new_session_number = int(file_name[12:14]) #   pick them out of the file name by grabbing an exact character number.\n","  \n","\n","\n","  # Check if I should add this session to the list of sessions\n","  should_add_session = True # Create a variable that by default should add the session\n","  for session in list_of_sessions: # Loop through the sessions in the list of sessions (list I may need to add it to)\n","    comp_subject_number = session.subject_number # Get both new session and subject number\n","    comp_session_number = session.session_number #   save them as comp_\n","    if (comp_session_number == new_session_number) and (comp_subject_number == new_subject_number): # If comp session and subject are equal to new subject and session\n","      should_add_session = False # In this case, that would mean that I should /not/ add the new session\n","\n","  if should_add_session: # If I should add the session\n","    newSession = Session(new_subject_number, new_session_number) # Initialize the new session object with the new subject and session values\n","    list_of_sessions.append(newSession) # Append the new session to the list of sessions\n","\n","# Now list_of_sessions has a list of Session objects for each of the sessions\n","\n","# Print out the list of the sessions in list_of_sessions:\n","# for session in list_of_sessions:\n","#   newStr = \"Subject: \" + str(session.subject_number) + \". Session: \" + str(session.session_number)\n","#   print(newStr) \n"],"metadata":{"id":"0nsD3egZzhsK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Code block created by Caleb\n","# Goal: Create a function that will take in an array of size [(length of file) x 6] and pad it with zeros for image size\n","\n","# This function pads with image width - 1 rows of zeros\n","def padData(data, image_width):\n","  dim = [image_width - 1,6] # The dimensions of the zeros will be the width of the image and six wide\n","  zeroArray = np.zeros(dim) # Create an array with zeros at the beginning of the correct size\n","  return np.concatenate((zeroArray, data)) # Concatonate the two arrays together with the zeros at the beginning\n","\n","# Print out the data for a small data array to see where the zeros go\n","data = np.array([[1, 2, 3, 4, 5, 6], [3, 4, 5, 6, 7, 5]])\n","paddedData = padData(data, 3) \n","print(data)"],"metadata":{"id":"KUAJ9FFgy2le","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648582659412,"user_tz":240,"elapsed":17,"user":{"displayName":"Trenton Wallis","userId":"12490720454166063115"}},"outputId":"81fa8a49-7c6c-4777-b93b-1a5636cc0642"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1 2 3 4 5 6]\n"," [3 4 5 6 7 5]]\n"]}]},{"cell_type":"code","source":["# Code block created by Caleb\n","# Goal: Step three of the whiteboard photo, so create \"images\" from padded data\n","\n","# This function takes in the padded data (an array  [(image_width + 40*session lenght).x 6] ) and an integer of the width of an image\n","def createImages(paddedData, image_width):\n","  all_images = [] # Create an empty array that will eventually hold all the images for this session's data\n","  number_of_images = paddedData.shape[0] - image_width + 1  # The number of images I should create is padded_data - (image_width - 1)\n","  for i in range(number_of_images): # Loop through i for each of the images I need to create\n","    all_images.append(paddedData[i:(i+image_width)])  # Grab out the array from the padded data equal to the image_width and starting at i, and append it to all images\n","  return np.asarray(all_images) \n","\n","\n","# This example uses the 'data' array from the code block above to pass into this function to see if it successfully creates two images\n","#   One should have two rows of zeros (because it gets all the padding) and the next should have one row of zeros\n","images = createImages(paddedData, 3)\n","print(images)"],"metadata":{"id":"TD-14JARy7g1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648582659413,"user_tz":240,"elapsed":14,"user":{"displayName":"Trenton Wallis","userId":"12490720454166063115"}},"outputId":"8d02b898-817e-4b81-8118-c15db4ec412b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [1. 2. 3. 4. 5. 6.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]\n","  [1. 2. 3. 4. 5. 6.]\n","  [3. 4. 5. 6. 7. 5.]]]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"HIglJIso0iTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# I will use the first session for this example:\n","example_session = list_of_sessions[0]\n","\n","example_x_data = example_session.getXDataFromFile()\n","example_x_time = example_session.getXTimeFromFile()\n","#example_y_data = example_session.getYDataFromFile() # y data does not exist here\n","example_y_time = example_session.getYTimeFromFile()\n","\n","# No need to extrapolate the ys \n","#xample_extrapolated_y = extrapolatedYs(example_x_time, example_y_time, example_y_data)\n","\n","# Now the x data and the extrapolate_y have the correct sizes, so y is the predictions we want\n","print(\"New y size, and then x data shape\")\n","\n","print(example_x_data.shape)\n","print(\"- - -\")\n","\n","# Now I will show the examples of transforming the x values into images. \n","example_image_width = 60 # You can play around with the width of the image\n","example_x_padded = padData(example_x_data, example_image_width)\n","print(\"With padding:\")\n","print(example_x_padded.shape)\n","print(\"- - -\")\n","\n","example_x_images = createImages(example_x_padded, example_image_width)\n","print(\"Shape of images, so should be back down to number of x values, with image_size x 6 secondary dimensions\")\n","print(example_x_images.shape)\n","print(\"- - -\")\n","print(\"Printing first images\")\n","# print(example_x_images[0])\n","# print(example_x_images[1])\n","\n","# So the data we're treating just like images for classification are:\n","# example_x_images          as the images\n","# example_extrapolated_y    as the labels"],"metadata":{"id":"_PUNpbVOz7TI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648582661976,"user_tz":240,"elapsed":2573,"user":{"displayName":"Trenton Wallis","userId":"12490720454166063115"}},"outputId":"740e0d41-3ede-494a-e212-43c53cd79d57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["New y size, and then x data shape\n","(37890, 6)\n","- - -\n","With padding:\n","(37949, 6)\n","- - -\n","Shape of images, so should be back down to number of x values, with image_size x 6 secondary dimensions\n","(37890, 60, 6)\n","- - -\n","Printing first images\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"jSf_V3gL0fys"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Code block created by Caleb\n","\n","def getImagesFromSession(session, image_width):\n","  image_width_number = int(image_width * 40) # This is the conversion rate between image_width in seconds and image_width the number of samples in an image\n","\n","  x_data = session.getXDataFromFile() # Get all the data for that session\n","  x_time = session.getXTimeFromFile()\n","  y_time = session.getYTimeFromFile()\n","\n","  \n","  x_padded = padData(x_data, image_width_number) # Pad the x values\n","  x_images = createImages(x_padded, image_width_number) # Then create images\n","\n","\n","\n","  print(\"Done processing session!\")\n","\n","\n","  return np.array(x_time), np.asarray(x_images), np.asarray(y_time)\n","\n","#Yt_train = Yt_train.type(torch.LongTensor)\n","# This next line calls the function to get the data for the model with the parameters passed in.\n","# Remember that image width and set length are in seconds\n","\n"],"metadata":{"id":"yLtJnh3O0OSr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["our_transform = transforms.Compose([\n","    # Converting RGB [0,255] to Tensor [0,1]\n","    transforms.ToTensor(),\n","    ])\n","\n","# def getYLabels(session, image_width, model):\n","#   x_time, x_images, y_time = getImagesFromSession(session, image_width)\n","#   model.eval()\n","#   test_data = CustomImageDataset(x_images, x_time, transform=our_transform) # x_time is just filler here \n","#   test_loader = torch.utils.data.DataLoader(test_data, batch_size=len(x_images))\n","#   _iter = iter(test_loader)\n","#   print(type(_iter))\n","#   data, index = next(_iter)\n","#   print(data.shape)\n","#   print(type(data))\n","#   predictions = model(data)\n","#   return x_time, x_images, y_time, predictions\n","\n","\n","def getYLabels(session, image_width, model):\n","  x_time, x_images, y_time = getImagesFromSession(session, image_width)\n","  predictions = []\n","\n","  model.eval()\n","  test_data = CustomImageDataset(x_images, x_time, transform=our_transform) # x_time is just filler here \n","  test_loader = torch.utils.data.DataLoader(test_data, batch_size=20)\n","  for data, _target in test_loader:\n","  \n","    output = model(data)\n","    output_numpy =  output.detach().numpy()\n","    for out in output_numpy:\n","      predictions.append(out)\n","\n","  return x_time, x_images, y_time, np.asarray(predictions)\n","\n","\n","  "],"metadata":{"id":"T4fLQZcaQV1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loadModelStateFromFile(model, filename):\n","  ''' Load model's state onto model passed in from a model file'''\n","  competition_path = \"/content/drive/Shareddrives/Neural Nets/Competition/Phase 1/Code/Models/\" #path to data folder in drive\n","  model_file_path = os.path.join(competition_path, filename+\".pt\")  #joing path and adding .npy to filename passed in\n","  print(model_file_path)\n","  model.load_state_dict(torch.load(model_file_path)) # Should not need a return since the model will be modified through its object \n"],"metadata":{"id":"QDV-y2u_S_9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom CNN\n","# Defining the CNN architecture\n","# Trenton's Net class\n","# class Net(nn.Module):\n","#   def __init__(self):\n","#     super(Net, self).__init__()\n","#     self.conv1 = nn.Conv2d(1, 6, 5, padding=2, padding_mode='zeros', dtype=float)\n","#    ## nn.MaxPool2d()\n","#     self.conv2 = nn.Conv2d(6, 16, 5, dtype=float)\n","#     self.fc1 = nn.Linear(16 * 56 * 2, 800, dtype=float)\n","#     self.fc2 = nn.Linear(800, 200, dtype=float) \n","#     self.fc3 = nn.Linear(200, 50, dtype=float)\n","#     self.fc4 = nn.Linear(50, 4, dtype=float)\n","#   def forward(self, x):\n","#     x = F.relu(self.conv1(x))\n","#     x = F.relu(self.conv2(x))\n","#     x = x.view(-1, 16 * 56 * 2) # At this point the feature map is 5 x 5 x 16\n","#     x = F.relu(self.fc1(x))\n","#     x = F.relu(self.fc2(x))\n","#     x = F.relu(self.fc3(x))\n","#     x = self.fc4(x)\n","#     return x\n","\n","\n","class Net(nn.Module):\n","  def __init__(self, ):\n","    super(Net, self).__init__()\n","    self.conv1 = nn.Conv2d(1, 4, 5, padding=2, padding_mode='zeros', dtype=float)\n","   ## nn.MaxPool2d()\n","    self.conv2 = nn.Conv2d(4, 8, 5, padding=2, padding_mode='zeros', dtype=float)\n","    self.drop2 = nn.Dropout2d(0.1)\n","    self.drop1 = nn.Dropout(0.1)\n","    self.batchNorm1 = nn.BatchNorm2d(4, dtype = float)\n","    self.batchNorm2 = nn.BatchNorm2d(8, dtype = float)\n","    self.batchNorm3 = nn.BatchNorm1d(512, dtype = float)\n","    self.batchNorm4 = nn.BatchNorm1d(128, dtype = float)\n","    self.batchNorm5 = nn.BatchNorm1d(32, dtype = float)\n","    self.fc1 = nn.Linear(8 * 40 * 6, 512, dtype=float)\n","    self.fc2 = nn.Linear(512, 128, dtype=float) \n","    self.fc3 = nn.Linear(128, 32, dtype=float)\n","    self.fc4 = nn.Linear(32, 4, dtype=float)\n","  def forward(self, x):\n","    x = self.drop2(F.relu(self.batchNorm1(self.conv1(x))))\n","    x = self.drop2(F.relu(self.batchNorm2(self.conv2(x))))\n","    x = x.view(-1, 8 * 40 * 6) # At this point the feature map is 5 x 5 x 16\n","    x = self.drop1(F.relu(self.batchNorm3(self.fc1(x))))\n","    x = self.drop1(F.relu(self.batchNorm4(self.fc2(x))))\n","    x = self.drop1(F.relu(self.batchNorm5(self.fc3(x))))\n","    x = self.fc4(x)\n","    return x\n"],"metadata":{"id":"Ay5vxZP3S0G-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test_session = list_of_sessions[0]\n","# test_model = Net()\n","\n","# loadModelStateFromFile(test_model, \"Test_Model\")\n","# test_x_time, test_x_data, test_y_time, test_predictions = getYLabels(test_session, 1.5, test_model)\n"],"metadata":{"id":"HTlguJmNSLRj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(test_predictions)\n","\n","# print(test_predictions[0])\n","\n","\n","# print(test_predictions[100][0])\n","# print(test_predictions[100][1])\n","# print(test_predictions[100][2])\n","# print(test_predictions[100][3])\n"],"metadata":{"id":"oJfSb-07k02L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# # for test_pred in test_predictions:\n","# #   print(test_pred.shape)\n","# _, max_indexes = torch.max(test_predictions, 1)\n","# #max_indexes.append(max_index)\n","\n","\n","\n","\n"],"metadata":{"id":"1mKIVf3X1H0t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(max_indexes[1000])\n","\n","# # for index in max_indexes:\n","# #   print(int(index))\n","# y_lined_up_with_x = []\n","# for index in max_indexes:\n","#   y_lined_up_with_x.append(int(index))\n","\n","# y_lined_up_with_x = np.asarray(y_lined_up_with_x)"],"metadata":{"id":"xT4qRwWf2gaJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# \"Downsample\" Y labels to correpond to what needs to be submitted\n","def deExtrapolateYs(x_time, y_time, y_lined_up_with_x):\n","  correctY = [] #Empty list for the newly transformed y values\n","  x_time_array = np.asarray(x_time)\n","  for y_t in y_time: # Loop over every y_time value\n","    index = (np.abs(x_time_array - y_t)).argmin() # Get index of closeset x_time to the y_t you are currently looking for\n","    correctY.append(y_lined_up_with_x[index]) # Add the y value at that index to the transformed y's\n","  return np.asarray(correctY)\n"],"metadata":{"id":"F_YFV96NT5K7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# actual_ys = deExtrapolateYs(test_x_time, test_y_time, y_lined_up_with_x)"],"metadata":{"id":"19X5WI0i3AQo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(actual_ys.shape)"],"metadata":{"id":"25P0HUBq3fs5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(actual_ys[0])"],"metadata":{"id":"wxsdfQp03rjl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pd.DataFrame(actual_ys).to_csv('first_y_perhaps')"],"metadata":{"id":"965soREA3yOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# np.savetxt('/content/drive/Shareddrives/Neural Nets/Competition/Phase 1/Code/Caleb Playing with It/first_y_save_txt', actual_ys)"],"metadata":{"id":"N48xa_vj4Z1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pd.DataFrame(actual_ys).to_csv('/content/drive/Shareddrives/Neural Nets/Competition/Phase 1/Code/Caleb Playing with It/first_y_maybe')"],"metadata":{"id":"yHTuxMwn5sEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# actual_ys.tofile('/content/drive/Shareddrives/Neural Nets/Competition/Phase 1/Code/Caleb Playing with It/first_y_maybe', sep='\\n')"],"metadata":{"id":"Ij7MCl5R5v1o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","loadModelStateFromFile(model, \"nathan_test\") # \"test_model\" for bad 77% model\n","\n","one_session = []\n","one_session.append(list_of_sessions[1])\n","\n","for session in list_of_sessions:\n","  x_time, x_data, y_time, predictions = getYLabels(session, 1, model) # This is 1.5 for bad 77% model\n","  predictions = torch.from_numpy(predictions)\n","  _, max_indexes = torch.max(predictions, 1)\n","\n","  y_lined_up_with_x = []\n","  for index in max_indexes:\n","    y_lined_up_with_x.append(int(index))\n","\n","  y_lined_up_with_x = np.asarray(y_lined_up_with_x)\n","\n","  y_values = deExtrapolateYs(x_time, y_time, y_lined_up_with_x)\n","\n","  y_values.tofile(f'/content/drive/Shareddrives/Neural Nets/Competition/Phase 1/Code/Caleb Playing with It/Nathan First Predictions/subject_{session.subject_number:03d}_{session.session_number:02d}__y_trash.csv', sep='\\n')\n","  y_lined_up_with_x.tofile(f'/content/drive/Shareddrives/Neural Nets/Competition/Phase 1/Data/Predictions/subject_{session.subject_number:03d}_{session.session_number:02d}__y.csv', sep='\\n')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"apfpGUe38FSe","executionInfo":{"status":"ok","timestamp":1648583157110,"user_tz":240,"elapsed":494738,"user":{"displayName":"Trenton Wallis","userId":"12490720454166063115"}},"outputId":"5dcd29b7-6b4c-4cb5-cd78-ad7cf32a93ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shareddrives/Neural Nets/Competition/Phase 1/Code/Models/nathan_test.pt\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n","Done processing session!\n"]}]},{"cell_type":"code","source":["print(predictions.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zpqKDetJXws","executionInfo":{"status":"ok","timestamp":1648583157110,"user_tz":240,"elapsed":10,"user":{"displayName":"Trenton Wallis","userId":"12490720454166063115"}},"outputId":"4b2508d3-2871-47bf-bc61-5abf0c896812"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([48138, 4])\n"]}]},{"cell_type":"code","source":["list_of_sessions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07rWenNdEUI5","executionInfo":{"status":"ok","timestamp":1648583157111,"user_tz":240,"elapsed":8,"user":{"displayName":"Trenton Wallis","userId":"12490720454166063115"}},"outputId":"51d537ac-55f7-49f5-81b9-9418e5eddfb1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<__main__.Session at 0x7fe74bec3b50>,\n"," <__main__.Session at 0x7fe74bec3a10>,\n"," <__main__.Session at 0x7fe74bec3a90>,\n"," <__main__.Session at 0x7fe74bec3a50>,\n"," <__main__.Session at 0x7fe74bec3990>,\n"," <__main__.Session at 0x7fe74bec3c10>,\n"," <__main__.Session at 0x7fe74bec3d10>,\n"," <__main__.Session at 0x7fe74bec3d50>,\n"," <__main__.Session at 0x7fe74bec3cd0>,\n"," <__main__.Session at 0x7fe74bec3b90>,\n"," <__main__.Session at 0x7fe74bec3c50>,\n"," <__main__.Session at 0x7fe74bec3c90>,\n"," <__main__.Session at 0x7fe74bec3bd0>,\n"," <__main__.Session at 0x7fe74bec3d90>,\n"," <__main__.Session at 0x7fe74bec3dd0>,\n"," <__main__.Session at 0x7fe74bec38d0>,\n"," <__main__.Session at 0x7fe74bec3910>,\n"," <__main__.Session at 0x7fe74bec3ed0>,\n"," <__main__.Session at 0x7fe74bec3f90>,\n"," <__main__.Session at 0x7fe74bec3e50>,\n"," <__main__.Session at 0x7fe74bec3e10>,\n"," <__main__.Session at 0x7fe74bec3e90>,\n"," <__main__.Session at 0x7fe74bec3f50>,\n"," <__main__.Session at 0x7fe74bec3fd0>,\n"," <__main__.Session at 0x7fe74bed4050>,\n"," <__main__.Session at 0x7fe74bed4090>,\n"," <__main__.Session at 0x7fe74bed40d0>,\n"," <__main__.Session at 0x7fe74bed4110>,\n"," <__main__.Session at 0x7fe74bed4150>]"]},"metadata":{},"execution_count":28}]}]}