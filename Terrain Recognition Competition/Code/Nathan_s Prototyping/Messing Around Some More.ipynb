{"cells":[{"cell_type":"markdown","metadata":{"id":"LISG7koflbSi"},"source":["Click to Edit Text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6nx1_NGmNCqi","executionInfo":{"status":"ok","timestamp":1649790930424,"user_tz":240,"elapsed":7818,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}},"outputId":"a3db01c5-1df8-4e68-e737-8361cdebee77"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","0\n","<torch.cuda.device object at 0x7f3dba403390>\n","1\n","Tesla T4\n"]}],"source":["# Brief check for using GPU\n","import torch\n","\n","print(torch.cuda.is_available())\n","\n","\n","print(torch.cuda.current_device())\n","\n","\n","print(torch.cuda.device(0))\n","\n","\n","print(torch.cuda.device_count())\n","\n","\n","print(torch.cuda.get_device_name(0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PH9k1pBld8O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649790931043,"user_tz":240,"elapsed":630,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}},"outputId":"2fc41fab-7fce-4e30-de8e-14ed31187ddc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sucessfully imported all of the necessary libraries!\n"]}],"source":["# necessary imports\n","import numpy as np\n","import csv\n","import os\n","from google.colab import drive \n","import matplotlib.pyplot as plt\n","import random # Added by Caleb\n","import math # Added by Caleb\n","import os\n","import torch\n","import pandas as pd\n","from torchvision.io import read_image\n","import torch\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","\n","print(\"Sucessfully imported all of the necessary libraries!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oaUXKnnrmClf","outputId":"1b988dc5-f62a-41dc-c20a-e30ce9c21c99","executionInfo":{"status":"ok","timestamp":1649790949503,"user_tz":240,"elapsed":18470,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":[" drive.mount('/content/drive/') ## mount to drive. This will ask for permission to access your Google drive each time"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CXzZAvEmmJrA","outputId":"854da371-eb09-4d5f-8d3c-e20ad6217eeb","scrolled":true,"executionInfo":{"status":"ok","timestamp":1649790952386,"user_tz":240,"elapsed":2892,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['subject_001_01__x.csv', 'subject_001_01__x_time.csv', 'subject_001_01__y.csv', 'subject_001_01__y_time.csv', 'subject_001_02__x.csv', 'subject_001_02__x_time.csv', 'subject_001_02__y.csv', 'subject_001_02__y_time.csv', 'subject_001_03__x.csv', 'subject_001_03__x_time.csv', 'subject_001_03__y.csv', 'subject_001_03__y_time.csv', 'subject_001_04__x.csv', 'subject_001_04__x_time.csv', 'subject_001_04__y.csv', 'subject_001_04__y_time.csv', 'subject_001_05__x.csv', 'subject_001_05__x_time.csv', 'subject_001_05__y.csv', 'subject_001_05__y_time.csv', 'subject_001_06__x.csv', 'subject_001_06__x_time.csv', 'subject_001_06__y.csv', 'subject_001_06__y_time.csv', 'subject_001_07__x.csv', 'subject_001_07__x_time.csv', 'subject_001_07__y.csv', 'subject_001_07__y_time.csv', 'subject_001_08__x.csv', 'subject_001_08__x_time.csv', 'subject_001_08__y.csv', 'subject_001_08__y_time.csv', 'subject_002_01__x.csv', 'subject_002_01__x_time.csv', 'subject_002_01__y.csv', 'subject_002_01__y_time.csv', 'subject_002_02__x.csv', 'subject_002_02__x_time.csv', 'subject_002_02__y.csv', 'subject_002_02__y_time.csv', 'subject_002_03__x.csv', 'subject_002_03__x_time.csv', 'subject_002_03__y.csv', 'subject_002_03__y_time.csv', 'subject_002_04__x.csv', 'subject_002_04__x_time.csv', 'subject_002_04__y.csv', 'subject_002_04__y_time.csv', 'subject_002_05__x.csv', 'subject_002_05__x_time.csv', 'subject_002_05__y.csv', 'subject_002_05__y_time.csv', 'subject_003_01__x.csv', 'subject_003_01__x_time.csv', 'subject_003_01__y.csv', 'subject_003_01__y_time.csv', 'subject_003_02__x.csv', 'subject_003_02__x_time.csv', 'subject_003_02__y.csv', 'subject_003_02__y_time.csv', 'subject_003_03__x.csv', 'subject_003_03__x_time.csv', 'subject_003_03__y.csv', 'subject_003_03__y_time.csv', 'subject_004_01__x.csv', 'subject_004_01__x_time.csv', 'subject_004_01__y.csv', 'subject_004_01__y_time.csv', 'subject_004_02__x.csv', 'subject_004_02__x_time.csv', 'subject_004_02__y.csv', 'subject_004_02__y_time.csv', 'subject_005_01__x.csv', 'subject_005_01__x_time.csv', 'subject_005_01__y.csv', 'subject_005_01__y_time.csv', 'subject_005_02__x.csv', 'subject_005_02__x_time.csv', 'subject_005_02__y.csv', 'subject_005_02__y_time.csv', 'subject_005_03__x.csv', 'subject_005_03__x_time.csv', 'subject_005_03__y.csv', 'subject_005_03__y_time.csv', 'subject_006_01__x.csv', 'subject_006_01__x_time.csv', 'subject_006_01__y.csv', 'subject_006_01__y_time.csv', 'subject_006_02__x.csv', 'subject_006_02__x_time.csv', 'subject_006_02__y.csv', 'subject_006_02__y_time.csv', 'subject_006_03__x.csv', 'subject_006_03__x_time.csv', 'subject_006_03__y.csv', 'subject_006_03__y_time.csv', 'subject_007_01__x.csv', 'subject_007_01__x_time.csv', 'subject_007_01__y.csv', 'subject_007_01__y_time.csv', 'subject_007_02__x.csv', 'subject_007_02__x_time.csv', 'subject_007_02__y.csv', 'subject_007_02__y_time.csv', 'subject_007_03__x.csv', 'subject_007_03__x_time.csv', 'subject_007_03__y.csv', 'subject_007_03__y_time.csv', 'subject_007_04__x.csv', 'subject_007_04__x_time.csv', 'subject_007_04__y.csv', 'subject_007_04__y_time.csv', 'subject_008_01__x.csv', 'subject_008_01__x_time.csv', 'subject_008_01__y.csv', 'subject_008_01__y_time.csv']\n"]}],"source":["data_folder_path = \"/content/drive/Shareddrives/Neural Nets/Competition/ECE542_sp2022_Project_TerrainRecognition/\" # path into Lobton's directory \n","type_of_data = \"TrainingData\" # Read in the type of data you want. Options are either:  'TrainingData' or 'TestData'\n","list_of_files = os.listdir(data_folder_path + type_of_data) # List everything in the directory at place 2022_Project_TerrainRecognition/TrainingData or /TestData (from line above)\n","list_of_files.sort() # Sort the list of files\n","print(list_of_files) # Print out the list of files\n","# need to load all this data in for augmentation (only the x data, but need to match what y data it connects with)\n","# we need to figure out what data we want to use too. "]},{"cell_type":"markdown","metadata":{"id":"OBkkBilDHwKB"},"source":["This next code block creates a session class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9tpL44Xba_a"},"outputs":[],"source":["# Code block created by Caleb\n","# Goal: Session class that holds subject and session number, and also returns the data for that session\n","\n","# This class can return the name of the file that holds the requested data\n","    # Example of how to use:\n","    # newSession = Session(4, 5)\n","    # newStr = newSession.xTimeName()\n","    # print(newStr)\n","    # # Prints out \"subject_004_05__x.csv\"\n","\n","class Session:\n","  def __init__(self, subject_number, session_number):\n","    self.subject_number = subject_number  # Initialize subject number\n","    self.session_number = session_number  # Initialize session number\n","\n","  # Each of the following member functions return the name of the specified file for that subject and session number of the Session:\n","\n","  def xTimeName(self):\n","    return \"subject_00\" + str(self.subject_number) + \"_0\" + str(self.session_number) + \"__x_time.csv\"\n","\n","  def yTimeName(self):\n","    return \"subject_00\" + str(self.subject_number) + \"_0\" + str(self.session_number) + \"__y_time.csv\"\n","\n","  def xDataName(self):\n","    return \"subject_00\" + str(self.subject_number) + \"_0\" + str(self.session_number) + \"__x.csv\"\n","\n","  def yDataName(self):\n","    return \"subject_00\" + str(self.subject_number) + \"_0\" + str(self.session_number) + \"__y.csv\"\n","\n","\n","  # This function input is the Session object that contians the subject and session numbers\n","  def getXDataFromFile(self):\n","    x_data_path = data_folder_path + \"TrainingData/\" + self.xDataName() # Get the path to the x_data file \n","    x_data = np.genfromtxt(x_data_path, delimiter=',')  # Read the data in from the text file\n","    return x_data  # Return the data array\n","    # This function returns the array of all the six x values\n","\n","\n","  def getYDataFromFile(self):\n","    y_data_path = data_folder_path + \"TrainingData/\" + self.yDataName() # Get the path to the x_data file \n","    y_data = np.genfromtxt(y_data_path, delimiter=',')  # Read the data in from the text file\n","    return y_data  # Return the data array\n","    # This function returns the array of just the y values\n","\n","  def getXTimeFromFile(self):\n","    x_time_path = data_folder_path + \"TrainingData/\" + self.xTimeName() # Get the path to the x_data file \n","    x_time = np.genfromtxt(x_time_path, delimiter=',')  # Read the data in from the text file\n","    return x_time  # Return the data array\n","    # This function returns the array of just the x time values\n","\n","  def getYTimeFromFile(self):\n","    y_time_path = data_folder_path + \"TrainingData/\" + self.yTimeName() # Get the path to the x_data file \n","    y_time = np.genfromtxt(y_time_path, delimiter=',')  # Read the data in from the text file\n","    return y_time  # Return the data array\n","    # This function returns the array of just the y time values\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B02t343MHzsD"},"source":["This next code block creates a list of sessions with all the sessions included in the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fk3CiC3ztgLV","outputId":"2002a953-0562-4a4a-ffbf-e8faf1e918bb","executionInfo":{"status":"ok","timestamp":1649790952388,"user_tz":240,"elapsed":33,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(29,)\n","8\n","1\n"]}],"source":["# Code block created by Caleb:\n","# Goal is to create list of each session, with a unique session id (session id made up of subject number and session number)\n","\n","list_of_sessions = []\n","\n","for file_name in list_of_files:\n","  # Extract subject and session number from file_name:\n","  new_subject_number = int(file_name[8:11]) # Since all provided subject and session numbers are single digit, we can \n","  new_session_number = int(file_name[12:14]) #   pick them out of the file name by grabbing an exact character number.\n","  \n","\n","\n","  # Check if I should add this session to the list of sessions\n","  should_add_session = True # Create a variable that by default should add the session\n","  for session in list_of_sessions: # Loop through the sessions in the list of sessions (list I may need to add it to)\n","    comp_subject_number = session.subject_number # Get both new session and subject number\n","    comp_session_number = session.session_number #   save them as comp_\n","    if (comp_session_number == new_session_number) and (comp_subject_number == new_subject_number): # If comp session and subject are equal to new subject and session\n","      should_add_session = False # In this case, that would mean that I should /not/ add the new session\n","\n","  if should_add_session: # If I should add the session\n","    newSession = Session(new_subject_number, new_session_number) # Initialize the new session object with the new subject and session values\n","    list_of_sessions.append(newSession) # Append the new session to the list of sessions\n","\n","# Now list_of_sessions has a list of Session objects for each of the sessions\n","print(np.asarray(list_of_sessions).shape)\n","print(list_of_sessions[28].subject_number)\n","print(list_of_sessions[28].session_number)\n","# Print out the list of the sessions in list_of_sessions:\n","# for session in list_of_sessions:\n","#   newStr = \"Subject: \" + str(session.subject_number) + \". Session: \" + str(session.session_number)\n","#   print(newStr) \n"]},{"cell_type":"markdown","metadata":{"id":"sc3CF6PwIB0i"},"source":["This next code block pads the data. This means step two is now complete. (Steps can be seen in this photo: https://drive.google.com/file/d/1dQfBs5h9Dca7GaQmbXdu0xfJZGLYvUru/view?usp=sharing)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PzsxQwJV6Uil","outputId":"aec89c47-3626-4eaf-a694-2ed6b13be6e0","executionInfo":{"status":"ok","timestamp":1649790952389,"user_tz":240,"elapsed":31,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1 2 3 4 5 6]\n"," [3 4 5 6 7 5]]\n"]}],"source":["# Code block created by Caleb\n","# Goal: Create a function that will take in an array of size [(length of file) x 6] and pad it with zeros for image size\n","\n","# This function pads with image width - 1 rows of zeros\n","def padData(data, image_width):\n","  dim = [image_width - 1,6] # The dimensions of the zeros will be the width of the image and six wide\n","  zeroArray = np.zeros(dim) # Create an array with zeros at the beginning of the correct size\n","  return np.concatenate((zeroArray, data)) # Concatonate the two arrays together with the zeros at the beginning\n","\n","# Print out the data for a small data array to see where the zeros go\n","data = np.array([[1, 2, 3, 4, 5, 6], [3, 4, 5, 6, 7, 5]])\n","paddedData = padData(data, 3) \n","print(data)"]},{"cell_type":"markdown","metadata":{"id":"b6q8eOU1JsEB"},"source":["Now, this next block needs to create \"images\" that have a certain definable width, and those will be of dimension [image_width x 6].\n","\n","There should be 40\\*(session_length) number of images, so the output of this will be a three-dimensional array [(40\\*session_length x image_width x 6]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LjntFYD9JsQx","outputId":"18f6c05e-da30-4bbe-f208-3ccf10cc22fb","executionInfo":{"status":"ok","timestamp":1649790952390,"user_tz":240,"elapsed":21,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [1. 2. 3. 4. 5. 6.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]\n","  [1. 2. 3. 4. 5. 6.]\n","  [3. 4. 5. 6. 7. 5.]]]\n"]}],"source":["# Code block created by Caleb\n","# Goal: Step three of the whiteboard photo, so create \"images\" from padded data\n","\n","# Work on a variable image stride length\n","\n","# This function takes in the padded data (an array  [(image_width + 40*session lenght).x 6] ) and an integer of the width of an image\n","def createImages(paddedData, image_width, image_stride):\n","  assert image_stride > 0, \"Error: Image stride length must be at least 1!\"\n","  all_images = [] # Create an empty array that will eventually hold all the images for this session's data\n","  possible_num_images = paddedData.shape[0] - image_width + 1  # The number of images I should create is padded_data - (image_width - 1)\n","  # print(\"Number of possible images: \" + str(possible_num_images))  \n","  for i in range(0, possible_num_images, image_stride): # Loop through i for each of the images I need to create\n","    all_images.append(paddedData[i:(i+image_width)])  # Grab out the array from the padded data equal to the image_width and starting at i, and append it to all images\n","  # print(len(all_images))\n","  return np.asarray(all_images)\n","\n","\n","\n","\n","# This example uses the 'data' array from the code block above to pass into this function to see if it successfully creates two images\n","#   One should have two rows of zeros (because it gets all the padding) and the next should have one row of zeros\n","images = createImages(paddedData, 3, 1)\n","print(images)"]},{"cell_type":"markdown","metadata":{"id":"Scxdt5X6mc2e"},"source":["This next code block will take in an x_time values, y_time values, and y values, and outputs a new list of y values that line up to time with the x_time places.\n","\n","In order to do this, it will take all x_time values, and for each value of y_time that is closest to the x_time it will add the y value for that y_time value to a new list. It then returns that new list.\n","\n","Note: we asked Dr. Lobaton what rate the camera took frames, and he said that the camera was 10hz. The students tried for the accuracy of the labels to be per frame (so 10hz what we were given), but he said we shouldn't consider the accuracy to be better than 200ms."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uvm0mv_qmdA0","outputId":"bee5fb10-53ef-4287-ca11-e8120cda5270","executionInfo":{"status":"ok","timestamp":1649790952650,"user_tz":240,"elapsed":271,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[1 4 4 3 2]\n"]}],"source":["# Block created by Caleb\n","# Goal: make a function that \"upsamples\" the y values. It will therefore get the closest-labelled y value for each x_time measurement value\n","\n","# This function takes in the x_time values and the y_time values, along with the y values\n","# For each of the x_time values, it finds the index of the y_time value that is closest.\n","# Then, it appends to the new y list the value of y at the index where the closest y_time value was found\n","def extrapolatedYs(x_time, y_time, y, image_stride_size):\n","  transformedY = [] #Empty list for the newly transformed y values\n","  y_time_array = np.asarray(y_time) # Create the y_time values as an array\n","  assert image_stride_size > 0, \"Error: Step size must be at least one!\"\n","  for x_t in range(0, len(x_time), image_stride_size): # Loop over every x_time value\n","    index = (np.abs(y_time_array - x_time[x_t])).argmin() # I found this online to get the index of the closest value from the y_time arrays\n","    transformedY.append(y[index]) # Add the y value at that index to the transformed y's\n","\n","  \n","  return np.asarray(transformedY)\n","# This example shows how to use this function above\n","my_x_time = [0, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.3, 0.325]\n","my_y_time = [          0.02,                     .12,                     .22,        .32]\n","my_y =  [1,     4,   3,   2]\n","my_image_step_size = 3\n","\n","new_y = extrapolatedYs(my_x_time, my_y_time, my_y, my_image_step_size)\n","print(new_y)"]},{"cell_type":"markdown","metadata":{"id":"PLkckpdhvix-"},"source":["I figured this is a good point to create just an example block for how to use the code that is above only.\n","\n","The final output at the bottom are all the images from a single session\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Oi5BYPSviYa","outputId":"95cd29d8-ce61-41fd-cca4-e311b3566b96","executionInfo":{"status":"ok","timestamp":1649790954950,"user_tz":240,"elapsed":2303,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["New y size, and then x data shape\n","(18945,)\n","(37890, 6)\n","- - -\n","With padding:\n","(37949, 6)\n","- - -\n","Image Stride Size: 2\n","Shape of images, so should be back down to number of x values / image stride size, with image_size x 6 secondary dimensions\n","(18945, 60, 6)\n","- - -\n","Printing first images\n"]}],"source":["# I will use the first session for this example:\n","example_session = list_of_sessions[0]\n","\n","example_x_data = example_session.getXDataFromFile()\n","example_x_time = example_session.getXTimeFromFile()\n","example_y_data = example_session.getYDataFromFile()\n","example_y_time = example_session.getYTimeFromFile()\n","\n","example_image_stride = 2\n","\n","# Now I will extrapolate out the y's:\n","example_extrapolated_y = extrapolatedYs(example_x_time, example_y_time, example_y_data, example_image_stride)\n","\n","# Now the x data and the extrapolate_y have the correct sizes, so y is the predictions we want\n","print(\"New y size, and then x data shape\")\n","print(example_extrapolated_y.shape)\n","print(example_x_data.shape)\n","print(\"- - -\")\n","\n","# Now I will show the examples of transforming the x values into images. \n","example_image_width = 60 # You can play around with the width of the image\n","example_x_padded = padData(example_x_data, example_image_width)\n","print(\"With padding:\")\n","print(example_x_padded.shape)\n","print(\"- - -\")\n","\n","example_x_images = createImages(example_x_padded, example_image_width, example_image_stride)\n","print(\"Image Stride Size: \" + str(example_image_stride))\n","print(\"Shape of images, so should be back down to number of x values / image stride size, with image_size x 6 secondary dimensions\")\n","print(example_x_images.shape)\n","print(\"- - -\")\n","print(\"Printing first images\")\n","# print(example_x_images[0])\n","# print(example_x_images[1])\n","\n","# So the data we're treating just like images for classification are:\n","# example_x_images          as the images\n","# example_extrapolated_y    as the labels"]},{"cell_type":"markdown","metadata":{"id":"RQjdH_7OP3Dk"},"source":["The code block below will solve Step 4 from the picture, which is splitting the images up into Sets. \n","\n","Edit Mar 23rd:\n","After talking with Dr. Lobaton, Nathan and I found out that the y values were created from a camera frame rate of y, so I've already created a function above that extrapolates y values out to have the same number of x values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iyB4-csU1ykN"},"outputs":[],"source":["# Code block created by Caleb\n","\n","# Becuase a set needs to have both the images and the associated labels (y values)\n","#   I was thinking of creating a set class that just has arrays of the images and y values, that way we can keep the two types of data together within the sets\n","class Set:\n","  def __init__(self, images, labels):\n","    self.images = images  # Initialize subject number\n","    self.labels = labels  # Initialize session number\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkoII3_qP3VH"},"outputs":[],"source":["# Code block created by Caleb\n","# Goal: create a function that creates the sets\n","\n","# Pass in the set length. This is the number of images per set\n","# Also pass in image_width\n","# images and labels (y_data) need to have the same length \n","# The dimensions of a set are \n","# Returns an array of the sets from that data\n","\n","# This is the function I am most concerned is not correct, so handle with care, lol\n","\n","\n","def createSets(images, labels, set_length, image_width, set_overlap):\n","  list_of_sets = []\n","  set_creation_step = math.ceil(image_width * (1 - set_overlap))\n","  num_of_images = images.shape[0] # These are how many images there are\n","  # print(\"Number of images in set creation function: \" + str(num_of_images))\n","  num_of_labels = labels.size\n","  assert(num_of_images == num_of_labels),\"The number of images does not match the number of correponding labels. This error is in createSets.\"\n","\n","\n","  i = 0 # Counter for which image about to get. This starts at the zeroth image I am going to put into a set first\n","  while (num_of_images - i) >= set_length: # Loop until can't make any more sets. If there are not enough images left to make another whole set, stop making tests\n","      \n","    new_set = Set(images[i:(i + set_length)], labels[i:(i + set_length)]) # Create a new set with as many images as are supposed to be in the set. has both images and labels\n","    list_of_sets.append(new_set) # Append this new set to the list of sets\n","\n","    # Now I need to increment i:\n","    i = i + set_length  # Increment past what I just added to the set, which is the set size\n","    # Increment i to ignore image width - 1 more images as the buffer between sets\n","    ignored_images_for_padding = set_creation_step - 1 \n","    i = i + ignored_images_for_padding\n","    \n","  return np.asarray(list_of_sets)\n","  \n","\n","\n","# # This example shows how to use this function above\n","# my_x_time = [0, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.275, 0.3]\n","# my_x_data = [[1, 2, 3, 4, 5, 6], [2, 3, 4, 5, 6, 7], [3, 4, 5, 6, 7, 8], [1, 2, 2, 4, 5, 6], [9, 2, 3, 4, 5, 6], [1, 3, 3, 2, 5, 6], [1, 2, 4, 4, 7, 6], [1, 2, 2, 4, 9, 9], [4, 4, 4, 4, 4, 4], [7, 2, 2, 4, 5, 6], [3, 3, 3, 4, 3, 6], [1, 2, 2, 4, 2, 2], [10, 22, 33, 24, 50, 6] ]\n","# my_y_time = [          0.02,                     .12,                     .22,        .32]\n","# my_y =  [1,     4,   3,   2]\n","# my_extrapolated_y = extrapolatedYs(my_x_time, my_y_time, my_y)\n","# my_image_width = 3 # You can play around with the width of the image\n","# my_x_padded = padData(my_x_data, my_image_width)\n","# my_x_images = createImages(my_x_padded, my_image_width)\n","\n","# my_set_size = 4\n","\n","# my_sets = createSets(my_x_images, my_extrapolated_y, my_set_size, my_image_width)\n","\n","#print(my_sets.shape)\n","\n","#for singleSet in my_sets:\n","#  print(singleSet.images)\n","#  print(singleSet.labels)\n","\n","# example_x_images\n","# example_image_width\n","# example_extrapolated_y\n","# example_set_size = 800\n","# example_set_step = 2\n","\n","# example_sets = createSets(example_x_images, example_extrapolated_y, example_set_size, example_image_width)\n","# print(example_x_images.shape)\n","# print(example_image_width)\n","# print(example_sets.shape)\n","#print(example_sets[0].images)\n","#print(example_sets[1].images)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JMnHJxpYv5at"},"source":["Now we need a function that takes in the sets, and percentages for train and validation (test will be 100% - train - validation) and splits the sets up randomly in that order.\n","\n","I will take the floor of the (number of sets)\\*percentage. AKA, if there are 44 sets, and we want 40% test data I will make 17 test sets since 44*.4 = 17.6. Then, I will do the same floor function on the validation amount, and then take the remaining ones for testing. So that means there will most likely be between 1 and 2 more test sets than we \"want\" based on the percentages we chose.\n","\n","I also made it so that we can pass in the seed so that we can control that and always get the same sets as we go along."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOpDuelzv5jK"},"outputs":[],"source":["# Code block created by Caleb\n","\n","def splitUpSets(list_of_sets, percentage_train, percentage_val, seed):\n","  training_sets = [] # Create empty lists for the lists of sets that will be returned\n","  validation_sets = []\n","  testing_sets = []\n","  num_of_sets = list_of_sets.size # Get the total number of sets passed into the function\n","  num_of_train_sets = math.floor(num_of_sets*percentage_train/100) # Take the floor of the percentage of each\n","  num_of_val_sets = math.floor(num_of_sets*percentage_val/100)\n","\n","  print(\"Num sets: \" + str(num_of_sets))\n","  print(\"Num Training Sets: \" + str(num_of_train_sets))\n","  print(\"Num Validation Sets: \" + str(num_of_val_sets))\n","  print(\"Num Test Sets: \" + str(num_of_sets - num_of_train_sets - num_of_val_sets))\n","  random.seed(seed) # Set the random seed based upon the seed passed in\n","\n","  for i in range(num_of_train_sets): # Loop over the number of train sets\n","    index_to_remove = random.randrange(0, num_of_sets, 1) # Find a random index to remove\n","    training_sets.append(list_of_sets[index_to_remove]) # Append the set to training_sets\n","    list_of_sets = np.delete(list_of_sets, index_to_remove)  # Remove the set from the master list\n","    # Decrement number of sets\n","    num_of_sets -= 1\n","\n","  for i in range(num_of_val_sets): # Loop over the number of train sets\n","    index_to_remove = random.randrange(0, num_of_sets, 1) # Find a random index to remove\n","    validation_sets.append(list_of_sets[index_to_remove]) # Append the set to validation_sets\n","    list_of_sets = np.delete(list_of_sets, index_to_remove) # Remove the set from the master list\n","    # Decrement number of sets\n","    num_of_sets -= 1\n","  \n","  testing_sets = list_of_sets # Training gets whatever is left\n","\n","  return np.asarray(training_sets), np.asarray(validation_sets), np.asarray(testing_sets) # Return arrays of the three types\n","\n","    \n","\n","\n","# example_train, example_val, example_test = splitUpSets(example_sets, 40, 30, 10, 0)\n","\n","# print(example_train[3].labels)\n","# print(example_val[1].labels)\n","# print(example_test[3].labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"io87BkwmNCqr"},"outputs":[],"source":["# '''\n","# Testing block for set creation\n","# '''\n","\n","# print(\"Processing Subject: \" + str(session.subject_number) + \". Session: \" + str(session.session_number))\n","\n","#    # I will use the first session for this example:\n","# example_session = list_of_sessions[0]\n","\n","# example_x_data = example_session.getXDataFromFile()\n","# example_x_time = example_session.getXTimeFromFile()\n","# example_y_data = example_session.getYDataFromFile()\n","# example_y_time = example_session.getYTimeFromFile()\n","\n","# example_image_stride = 4\n","\n","# # Now I will extrapolate out the y's:\n","# example_extrapolated_y = extrapolatedYs(example_x_time, example_y_time, example_y_data, example_image_stride)\n","\n","# # Now the x data and the extrapolate_y have the correct sizes, so y is the predictions we want\n","# print(\"New y size, and then x data shape\")\n","# print(example_extrapolated_y.shape)\n","# print(example_x_data.shape)\n","# print(\"- - -\")\n","\n","# # Now I will show the examples of transforming the x values into images. \n","# example_image_width = 60 # You can play around with the width of the image\n","# example_x_padded = padData(example_x_data, example_image_width)\n","# print(\"With padding:\")\n","# print(example_x_padded.shape)\n","# print(\"- - -\")\n","\n","# example_x_images = createImages(example_x_padded, example_image_width, example_image_stride)\n","# print(\"Image Stride Size: \" + str(example_image_stride))\n","# print(\"Shape of images, so should be back down to number of x values / image stride size, with image_size x 6 secondary dimensions\")\n","# print(example_x_images.shape)\n","# print(\"- - -\")\n","\n","# # print(example_x_images[0])\n","# # print(example_x_images[1])\n","\n","# # So the data we're treating just like images for classification are:\n","# # example_x_images          as the images\n","# # example_extrapolated_y    as the labels\n","\n","# example_set_size = 800\n","# example_set_step = 1\n","# print(example_x_images.shape)\n","# print(example_image_width)\n","# example_sets = createSets(example_x_images, example_extrapolated_y, example_set_size, example_image_width)\n","# print(example_sets.shape)"]},{"cell_type":"code","source":["# ########### EXPERIMENTAL ###############\n","# import keras\n","# from keras.models import Sequential\n","# from keras.layers import Dense\n","# from keras.layers import LSTM\n","# from keras.layers import Dropout\n","# import pandas as pd\n","\n","# dataset_train = pd.read_csv('/content/drive/Shareddrives/Neural Nets/Competition/ECE542_sp2022_Project_TerrainRecognition/TrainingData')\n","\n","# def ExperimentalFunction(list_of_sessions, image_width, image_stride_size):\n","#   image_width_number = int(image_width * 40) # This is the conversion rate between image_width in seconds and image_width the number of samples in an image\n","#   assert image_width > 0, \"Error: Image width must be at least 1!\"\n","#   assert image_stride_size > 0, \"Error: Image width must be at least 1!\"\n","#   x_train = [] # Initialize the lists to zero\n","#   y_train = []\n","#   x_val = []\n","#   y_val = []\n","#   x_test = []\n","#   y_test = []\n","\n","#   # Iterate over each sessoin in the list of sessions:\n","#   for session in list_of_sessions:\n","#     #strToPrint = \"Processing Subject: \" + str(session.subject_number) + \". Session: \" + str(session.session_number)\n","#     print(\"Processing Subject: \" + str(session.subject_number) + \". Session: \" + str(session.session_number))\n","\n","#     # Take a session and get a list of sets\n","\n","#     x_data = session.getXDataFromFile() # Get all the data for that session\n","#     x_time = session.getXTimeFromFile()\n","#     y_data = session.getYDataFromFile()\n","#     y_time = session.getYTimeFromFile()\n","    \n","\n","#     extrapolated_y = extrapolatedYs(x_time, y_time, y_data, image_stride_size) # Now I will extrapolate out the y's:\n","    \n","\n","#     x_padded = padData(x_data, image_width_number) # Pad the x values\n","#     x_images = createImages(x_padded, image_width_number, image_stride_size) # Then create images\n","\n","# X_train = np.reshape(X_train, ())"],"metadata":{"id":"o-rmIhLWXgq_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTAHmAGp46Ap"},"source":["Now process all the data and make HUGE arrays with all of the data.\n","\n","Pass in all the hyperparameters we made"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13Lfskdr45gz","outputId":"80c0a405-ccd7-4552-dc58-c8bbe07fd831","scrolled":false,"executionInfo":{"status":"ok","timestamp":1649791017717,"user_tz":240,"elapsed":62773,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing Subject: 1. Session: 1\n","Shape of x_images:\n","(37890, 60, 6)\n","Shape of extrapolated y:\n","(37890,)\n","Number of sets:\n","(44,)\n","Num sets: 44\n","Num Training Sets: 37\n","Num Validation Sets: 6\n","Num Test Sets: 1\n","Processing Subject: 1. Session: 2\n","Shape of x_images:\n","(70172, 60, 6)\n","Shape of extrapolated y:\n","(70172,)\n","Number of sets:\n","(81,)\n","Num sets: 81\n","Num Training Sets: 68\n","Num Validation Sets: 12\n","Num Test Sets: 1\n","Processing Subject: 1. Session: 3\n","Shape of x_images:\n","(43146, 60, 6)\n","Shape of extrapolated y:\n","(43146,)\n","Number of sets:\n","(50,)\n","Num sets: 50\n","Num Training Sets: 42\n","Num Validation Sets: 7\n","Num Test Sets: 1\n","Processing Subject: 1. Session: 4\n","Shape of x_images:\n","(54956, 60, 6)\n","Shape of extrapolated y:\n","(54956,)\n","Number of sets:\n","(64,)\n","Num sets: 64\n","Num Training Sets: 54\n","Num Validation Sets: 9\n","Num Test Sets: 1\n","Processing Subject: 1. Session: 5\n","Shape of x_images:\n","(59121, 60, 6)\n","Shape of extrapolated y:\n","(59121,)\n","Number of sets:\n","(68,)\n","Num sets: 68\n","Num Training Sets: 57\n","Num Validation Sets: 10\n","Num Test Sets: 1\n","Processing Subject: 1. Session: 6\n","Shape of x_images:\n","(38631, 60, 6)\n","Shape of extrapolated y:\n","(38631,)\n","Number of sets:\n","(45,)\n","Num sets: 45\n","Num Training Sets: 38\n","Num Validation Sets: 6\n","Num Test Sets: 1\n","Processing Subject: 1. Session: 7\n","Shape of x_images:\n","(31423, 60, 6)\n","Shape of extrapolated y:\n","(31423,)\n","Number of sets:\n","(36,)\n","Num sets: 36\n","Num Training Sets: 30\n","Num Validation Sets: 5\n","Num Test Sets: 1\n","Processing Subject: 1. Session: 8\n","Shape of x_images:\n","(66762, 60, 6)\n","Shape of extrapolated y:\n","(66762,)\n","Number of sets:\n","(77,)\n","Num sets: 77\n","Num Training Sets: 65\n","Num Validation Sets: 11\n","Num Test Sets: 1\n","Processing Subject: 2. Session: 1\n","Shape of x_images:\n","(55999, 60, 6)\n","Shape of extrapolated y:\n","(55999,)\n","Number of sets:\n","(65,)\n","Num sets: 65\n","Num Training Sets: 55\n","Num Validation Sets: 9\n","Num Test Sets: 1\n","Processing Subject: 2. Session: 2\n","Shape of x_images:\n","(55401, 60, 6)\n","Shape of extrapolated y:\n","(55401,)\n","Number of sets:\n","(64,)\n","Num sets: 64\n","Num Training Sets: 54\n","Num Validation Sets: 9\n","Num Test Sets: 1\n","Processing Subject: 2. Session: 3\n","Shape of x_images:\n","(48797, 60, 6)\n","Shape of extrapolated y:\n","(48797,)\n","Number of sets:\n","(56,)\n","Num sets: 56\n","Num Training Sets: 47\n","Num Validation Sets: 8\n","Num Test Sets: 1\n","Processing Subject: 2. Session: 4\n","Shape of x_images:\n","(51461, 60, 6)\n","Shape of extrapolated y:\n","(51461,)\n","Number of sets:\n","(59,)\n","Num sets: 59\n","Num Training Sets: 50\n","Num Validation Sets: 8\n","Num Test Sets: 1\n","Processing Subject: 2. Session: 5\n","Shape of x_images:\n","(42319, 60, 6)\n","Shape of extrapolated y:\n","(42319,)\n","Number of sets:\n","(49,)\n","Num sets: 49\n","Num Training Sets: 41\n","Num Validation Sets: 7\n","Num Test Sets: 1\n","Processing Subject: 3. Session: 1\n","Shape of x_images:\n","(36313, 60, 6)\n","Shape of extrapolated y:\n","(36313,)\n","Number of sets:\n","(42,)\n","Num sets: 42\n","Num Training Sets: 35\n","Num Validation Sets: 6\n","Num Test Sets: 1\n","Processing Subject: 3. Session: 2\n","Shape of x_images:\n","(47210, 60, 6)\n","Shape of extrapolated y:\n","(47210,)\n","Number of sets:\n","(55,)\n","Num sets: 55\n","Num Training Sets: 46\n","Num Validation Sets: 8\n","Num Test Sets: 1\n","Processing Subject: 3. Session: 3\n","Shape of x_images:\n","(19533, 60, 6)\n","Shape of extrapolated y:\n","(19533,)\n","Number of sets:\n","(22,)\n","Num sets: 22\n","Num Training Sets: 18\n","Num Validation Sets: 3\n","Num Test Sets: 1\n","Processing Subject: 4. Session: 1\n","Shape of x_images:\n","(34309, 60, 6)\n","Shape of extrapolated y:\n","(34309,)\n","Number of sets:\n","(40,)\n","Num sets: 40\n","Num Training Sets: 34\n","Num Validation Sets: 6\n","Num Test Sets: 0\n","Processing Subject: 4. Session: 2\n","Shape of x_images:\n","(34476, 60, 6)\n","Shape of extrapolated y:\n","(34476,)\n","Number of sets:\n","(40,)\n","Num sets: 40\n","Num Training Sets: 34\n","Num Validation Sets: 6\n","Num Test Sets: 0\n","Processing Subject: 5. Session: 1\n","Shape of x_images:\n","(56079, 60, 6)\n","Shape of extrapolated y:\n","(56079,)\n","Number of sets:\n","(65,)\n","Num sets: 65\n","Num Training Sets: 55\n","Num Validation Sets: 9\n","Num Test Sets: 1\n","Processing Subject: 5. Session: 2\n","Shape of x_images:\n","(34519, 60, 6)\n","Shape of extrapolated y:\n","(34519,)\n","Number of sets:\n","(40,)\n","Num sets: 40\n","Num Training Sets: 34\n","Num Validation Sets: 6\n","Num Test Sets: 0\n","Processing Subject: 5. Session: 3\n","Shape of x_images:\n","(44905, 60, 6)\n","Shape of extrapolated y:\n","(44905,)\n","Number of sets:\n","(52,)\n","Num sets: 52\n","Num Training Sets: 44\n","Num Validation Sets: 7\n","Num Test Sets: 1\n","Processing Subject: 6. Session: 1\n","Shape of x_images:\n","(48125, 60, 6)\n","Shape of extrapolated y:\n","(48125,)\n","Number of sets:\n","(56,)\n","Num sets: 56\n","Num Training Sets: 47\n","Num Validation Sets: 8\n","Num Test Sets: 1\n","Processing Subject: 6. Session: 2\n","Shape of x_images:\n","(59562, 60, 6)\n","Shape of extrapolated y:\n","(59562,)\n","Number of sets:\n","(69,)\n","Num sets: 69\n","Num Training Sets: 58\n","Num Validation Sets: 10\n","Num Test Sets: 1\n","Processing Subject: 6. Session: 3\n","Shape of x_images:\n","(45129, 60, 6)\n","Shape of extrapolated y:\n","(45129,)\n","Number of sets:\n","(52,)\n","Num sets: 52\n","Num Training Sets: 44\n","Num Validation Sets: 7\n","Num Test Sets: 1\n","Processing Subject: 7. Session: 1\n","Shape of x_images:\n","(46201, 60, 6)\n","Shape of extrapolated y:\n","(46201,)\n","Number of sets:\n","(53,)\n","Num sets: 53\n","Num Training Sets: 45\n","Num Validation Sets: 7\n","Num Test Sets: 1\n","Processing Subject: 7. Session: 2\n","Shape of x_images:\n","(46989, 60, 6)\n","Shape of extrapolated y:\n","(46989,)\n","Number of sets:\n","(54,)\n","Num sets: 54\n","Num Training Sets: 45\n","Num Validation Sets: 8\n","Num Test Sets: 1\n","Processing Subject: 7. Session: 3\n","Shape of x_images:\n","(44641, 60, 6)\n","Shape of extrapolated y:\n","(44641,)\n","Number of sets:\n","(52,)\n","Num sets: 52\n","Num Training Sets: 44\n","Num Validation Sets: 7\n","Num Test Sets: 1\n","Processing Subject: 7. Session: 4\n","Shape of x_images:\n","(39439, 60, 6)\n","Shape of extrapolated y:\n","(39439,)\n","Number of sets:\n","(45,)\n","Num sets: 45\n","Num Training Sets: 38\n","Num Validation Sets: 6\n","Num Test Sets: 1\n","Processing Subject: 8. Session: 1\n","Shape of x_images:\n","(48138, 60, 6)\n","Shape of extrapolated y:\n","(48138,)\n","Number of sets:\n","(56,)\n","Num sets: 56\n","Num Training Sets: 47\n","Num Validation Sets: 8\n","Num Test Sets: 1\n","Done processing all sessions!\n"]}],"source":["# Code block created by Caleb\n","\n","# Inputs:\n","#   list_of_sessions\n","#   image_width (in seconds)\n","#   set_length (in seconds)\n","#   training percentage\n","#   validation percentage\n","#   random number seed \n","def getDataForModel(list_of_sessions, image_width, image_stride_size, set_length, set_overlap, train_percentage, val_percentage, seed):\n","  image_width_number = int(image_width * 40) # This is the conversion rate between image_width in seconds and image_width the number of samples in an image\n","  set_length_number = int(set_length * 40) # Convert to number of images in a set\n","  assert image_width > 0, \"Error: Image width must be at least 1!\"\n","  assert image_stride_size > 0, \"Error: Image width must be at least 1!\"\n","  x_train = [] # Initialize the lists to zero\n","  y_train = []\n","  x_val = []\n","  y_val = []\n","  x_test = []\n","  y_test = []\n","\n","  # Create an array of the random seed values to pass in to the functions\n","  random.seed(seed) # Seed the random number generator with the value passed in \n","  random_seeds = []\n","  # For each of the lists of sessions, append a new random number to the random seeds\n","  for i in range(len(list_of_sessions)):\n","    random_seeds.append(random.randint(-999999999, 999999999)) # I chose these bounds completely arbitrariliy\n","\n","  index_of_random = 0 # I need a list to access the random numbers I just created\n","\n","  # Iterate over each sessoin in the list of sessions:\n","  for session in list_of_sessions:\n","    #strToPrint = \"Processing Subject: \" + str(session.subject_number) + \". Session: \" + str(session.session_number)\n","    print(\"Processing Subject: \" + str(session.subject_number) + \". Session: \" + str(session.session_number))\n","\n","    # Take a session and get a list of sets\n","\n","    x_data = session.getXDataFromFile() # Get all the data for that session\n","    x_time = session.getXTimeFromFile()\n","    y_data = session.getYDataFromFile()\n","    y_time = session.getYTimeFromFile()\n","    \n","\n","    extrapolated_y = extrapolatedYs(x_time, y_time, y_data, image_stride_size) # Now I will extrapolate out the y's:\n","    \n","     all_training_labels = np.array([0,1,2,3])\n","\n","    labels, counts = np.unique(all_training_labels, return_counts = True)\n","    totalCount = np.sum(counts)\n","    \n","\n","\n","\n","    x_padded = padData(x_data, image_width_number) # Pad the x values\n","    x_images = createImages(x_padded, image_width_number, image_stride_size) # Then create images\n","\n","\n","    \n","\n","    print(\"Shape of x_images:\")\n","    print(np.asarray(x_images).shape)\n","    print(\"Shape of extrapolated y:\")\n","    print(np.asarray(extrapolated_y).shape)\n","    \n","    # Then create sets\n","    list_of_sets = createSets(x_images, extrapolated_y, set_length_number, image_width_number, set_overlap)\n","    \n","    print(\"Number of sets:\")\n","    print(np.asarray(list_of_sets).shape)\n","\n","    # Get the three sets from this one\n","    train, val, test = splitUpSets(list_of_sets, train_percentage, val_percentage, random_seeds[index_of_random])\n","\n","    index_of_random += 1 # Make sure to increment index of random numbers since just used one of them\n","\n","    # Loop over training data, and append the images and labels\n","    for individualSet in train:\n","      for image in individualSet.images:\n","        x_train.append(image)\n","\n","      for label in individualSet.labels:\n","        y_train.append(label)\n","\n","\n","    # Loop over validation data, and append the images and labels\n","    for individualSet in val:\n","      for image in individualSet.images:\n","        x_val.append(image)\n","\n","      for label in individualSet.labels:\n","        y_val.append(label)\n","\n","\n","    # Loop over testing data, and append the images and labels\n","    for individualSet in test:\n","      for image in individualSet.images:\n","        x_test.append(image)\n","\n","      for label in individualSet.labels:\n","        y_test.append(label)\n","\n","    assert(np.asarray(x_train).shape[0] == len(y_train)),\"Wrong \"\n","    assert(np.asarray(x_val).shape[0] == len(y_val)),\"Wrong \"\n","    assert(np.asarray(x_test).shape[0] == len(y_test)),\"Wrong \"\n","\n","  print(\"Done processing all sessions!\")\n","  return np.asarray(x_train), np.asarray(y_train), np.asarray(x_val), np.asarray(y_val), np.asarray(x_test), np.asarray(y_test)\n","\n","#Yt_train = Yt_train.type(torch.LongTensor)\n","# This next line calls the function to get the data for the model with the parameters passed in.\n","# Remember that image width and set length are in seconds\n","# getDataForModel(list_of_sessions, image_width, image_step_size, set_length, set_overlap, train_percentage, val_percentage, seed)\n","x_train, y_train, x_val, y_val, x_test, y_test = getDataForModel(list_of_sessions, 1.5, 1, 20, 0.0, 85, 15, 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BwSDxXNPH1qf","outputId":"ce3727b1-a957-417c-b4ac-6c97b0c31e8f","executionInfo":{"status":"ok","timestamp":1649791017718,"user_tz":240,"elapsed":15,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(1044800, 60, 6)\n","(1044800,)\n","(175200, 60, 6)\n","(175200,)\n","(20800, 60, 6)\n","(20800,)\n","<class 'numpy.float64'>\n"]}],"source":["print(x_train.shape)\n","print(y_train.shape)\n","print(x_val.shape)\n","print(y_val.shape)\n","print(x_test.shape)\n","print(y_test.shape)\n","print(type(y_val[0]))\n"]},{"cell_type":"markdown","metadata":{"id":"Ut0dEQFSRJNZ"},"source":["So I am very hesitant to say this because of how unbelievably new I am to Python, but I *think* I may have actually gotten all the data augmentation done. Those six arrays which the sizes are printed above should contain all the data we need to feed into our convolutional neural network.\n","\n","Please take this with a grain of salt, because I probably have many, many bugs, but I did everything I could think of for it."]},{"cell_type":"markdown","metadata":{"id":"Aa7dWnRnIi-m"},"source":["The below code block was made by Trenton and prints the data for Subject 1, Session 1:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqSzYHc-mrCO"},"outputs":[],"source":["\n","# # pulling data from \n","# path_to_trainingdata = os.path.abspath(data_folder_path + type_of_data)\n","# subject_001_01__x = os.path.join(path_to_trainingdata,'subject_003_01__x.csv' )\n","# subject_001_01__x_time = os.path.join(path_to_trainingdata, 'subject_003_01__x_time.csv')\n","# subject_001_01__y = os.path.join(path_to_trainingdata,'subject_003_01__y.csv' )\n","# subject_001_01__y_time = os.path.join(path_to_trainingdata,'subject_003_01__y_time.csv' )\n","# # pull data from the file\n","# imu_x = np.genfromtxt(subject_001_01__x,delimiter=',')\n","# imu_y = np.genfromtxt(subject_001_01__y, delimiter='\\n')\n","# imu_x_time = np.genfromtxt(subject_001_01__x_time, delimiter='\\n')\n","# imu_y_time = np.genfromtxt(subject_001_01__y_time, delimiter='\\n')\n","# #create plot\n","# fig, axes = plt.subplots(2, 1)\n","\n","# axes[0].plot(imu_x_time, imu_x)\n","# axes[1].plot(imu_y_time, imu_y)"]},{"cell_type":"markdown","metadata":{"id":"Ek85UFQGk6Am"},"source":["CODE BASED ON HOMEWORK 5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NZtRNxNDrwot","outputId":"39fe5d07-e0cc-460f-b245-5c89e4e6604c","executionInfo":{"status":"ok","timestamp":1649791020305,"user_tz":240,"elapsed":2594,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Shuffling within sets!\n","torch.int64\n","<class 'torch.utils.data.dataloader._SingleProcessDataLoaderIter'>\n","torch.Size([10, 1, 6, 60])\n"]}],"source":["import os\n","import torch\n","import pandas as pd\n","from torchvision.io import read_image\n","import torch\n","from torch.utils.data import Dataset\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","import sklearn.utils\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, x_data, y_data, img_dir=None, transform=None, target_transform=None):\n","        self.img_labels = y_data\n","        self.img_dir = img_dir\n","        self.x_data = x_data\n","        self.y_data = y_data\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        image = self.x_data[idx]\n","        label = self.y_data[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label\n","\n","\n","print(\"Shuffling within sets!\") # Trying to improve performance by shuffling the images within each set\n","# np.random.shuffle(x_train)\n","# np.random.shuffle(x_val)\n","# np.random.shuffle(x_test)\n","\n","\n","# def unison_shuffled_copies(a, b):\n","#     assert len(x_train) == len(y_train)\n","#     p = np.random.permutation(len(a))\n","#     return a[p], b[p]\n","\n","#x_train_shuffled, y_train_shuffled = unison_shuffled_copies(x_train, y_train)\n","x_train_shuffled, y_train_shuffled = sklearn.utils.shuffle(x_train, y_train)\n","\n","\n","\n","our_transform = transforms.Compose([\n","    # Converting RGB [0,255] to Tensor [0,1]\n","    transforms.ToTensor(),\n","    ])\n","\n","batch_size = 10;\n","\n","\n","\n","def createNewSubSet(num_set, main_set_x, main_set_y):\n","  subset_x = []\n","  subset_y = []\n","  for i in range(num_set): # Loop over the number of sets\n","      index_to_add = random.randrange(0, len(main_set_x), 1) # Find a random index to remove\n","      subset_x.append(main_set_x[index_to_add]) # Append the set to training_sets\n","      subset_y.append(main_set_y[index_to_add])\n","  return np.asarray(subset_x), np.asarray(subset_y)\n","\n","\n","\n","x_train_len = len(x_train_shuffled)\n","x_val_len = len(x_val) \n","x_test_len = len(x_test)\n","\n","# small_x_train, small_y_train = createNewSubSet(x_train_len, x_train_shuffled, y_train_shuffled)\n","# small_x_val, small_y_val = createNewSubSet(x_val_len, x_val, y_val)\n","# small_x_test, small_y_test = createNewSubSet(x_test_len, x_test, y_test)\n","\n","\n","\n","x_train_re = np.reshape(x_train_shuffled, [x_train_shuffled.shape[0], 6, 60, 1])\n","x_val_re = np.reshape(x_val, [x_val.shape[0], 6, 60, 1])\n","x_test_re = np.reshape(x_test, [x_test.shape[0], 6, 60, 1])\n","\n","y_train_t = torch.tensor(y_train_shuffled).type(torch.LongTensor) \n","y_val_t = torch.tensor(y_val).type(torch.LongTensor) \n","y_test_t = torch.tensor(y_test).type(torch.LongTensor) \n","\n","\n","\n","print(y_train_t[0].dtype)\n","\n","train_data = CustomImageDataset(x_train_re, y_train_t, transform=our_transform)\n","valid_data = CustomImageDataset(x_val_re, y_val_t, transform=our_transform)\n","test_data = CustomImageDataset(x_test_re, y_test_t, transform=our_transform)\n","\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n","valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n","\n","\n","_iter = iter(train_loader)\n","print(type(_iter))\n","image_batch, index = next(_iter)\n","print(image_batch.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9zjvn2UTPryh","outputId":"30a8b3cd-9d2c-4f8e-9aca-48d95522f8dc","executionInfo":{"status":"ok","timestamp":1649791112238,"user_tz":240,"elapsed":140,"user":{"displayName":"Caleb Wheeler","userId":"02287818206377120960"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (conv1): Conv2d(1, 128, kernel_size=(6, 30), stride=(1, 1))\n","  (drop2): Dropout2d(p=0.5, inplace=False)\n","  (drop1): Dropout(p=0.2, inplace=False)\n","  (maxPool): MaxPool1d(kernel_size=1, stride=31, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=3968, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=4, bias=True)\n",")\n"]}],"source":["from torch._C import dtype\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# Defining the CNN architecture\n","# class Net(nn.Module):\n","#   def __init__(self, ):\n","#     super(Net, self).__init__()\n","#     self.conv1 = nn.Conv2d(1, 128, (6,30), dtype=float)\n","#     self.drop2 = nn.Dropout2d(0.5)\n","#     self.drop1 = nn.Dropout(0.2)\n","#     self.maxPool = nn.MaxPool1d(31, 1)\n","#     # self.batchNorm1 = nn.BatchNorm2d(128, dtype = float)\n","#     # self.batchNorm2 = nn.BatchNorm2d(16, dtype = float)\n","#     # self.batchNorm3 = nn.BatchNorm1d(800, dtype = float)\n","#     # self.batchNorm4 = nn.BatchNorm1d(200, dtype = float)\n","#     # self.batchNorm5 = nn.BatchNorm1d(50, dtype = float)\n","#     self.fc1 = nn.Linear(128 * 31 * 1, 128, dtype=float)\n","#     self.fc2 = nn.Linear(128, 64, dtype=float) \n","#     self.fc3 = nn.Linear(64, 32, dtype=float)\n","#     self.fc4 = nn.Linear(32, 4, dtype=float)\n","#   def forward(self, x):\n","#     x = self.drop2(F.relu(self.conv1(x)))\n","#     # x = x.view(-1, 128 * 30 * 6) \n","#     x = x.view(-1, 128 * 31 * 1) \n","#     # x= self.maxPool(x)\n","#     x = self.drop1(F.relu(self.fc1(x)))\n","#     x = self.drop1(F.relu(self.fc2(x)))\n","#     x = F.relu(self.fc3(x))\n","#     x = self.fc4(x)\n","#     return x\n","class Net(nn.Module):\n","  def __init__(self, ):\n","    super(Net, self).__init__()\n","    self.conv1 = nn.Conv2d(1, 128, (6, 30), dtype=float)\n","    self.drop2 = nn.Dropout2d(0.5)\n","    self.drop1 = nn.Dropout(0.2)\n","    self.maxPool = nn.MaxPool1d(1, 31)\n","    # self.batchNorm1 = nn.BatchNorm2d(128, dtype = float)\n","    # self.batchNorm2 = nn.BatchNorm2d(16, dtype = float)\n","    # self.batchNorm3 = nn.BatchNorm1d(800, dtype = float)\n","    # self.batchNorm4 = nn.BatchNorm1d(200, dtype = float)\n","    # self.batchNorm5 = nn.BatchNorm1d(50, dtype = float)\n","    self.fc1 = nn.Linear(128 * 31 * 1, 128, dtype=float)\n","    self.fc2 = nn.Linear(128, 64, dtype=float) \n","    self.fc3 = nn.Linear(64, 32, dtype=float)\n","    self.fc4 = nn.Linear(32, 4, dtype=float)\n","  def forward(self, x):\n","    x = self.drop2(F.relu(self.conv1(x)))\n","    # x = x.view(-1, 128 * 30 * 6) \n","    x = x.view(-1, 128 * 31 * 1) \n","    # x= self.maxPool(x)\n","    x = self.drop1(F.relu(self.fc1(x)))\n","    x = self.drop1(F.relu(self.fc2(x)))\n","    x = F.relu(self.fc3(x))\n","    x = self.fc4(x)\n","    return x\n","\n","# Create a complete CNN\n","model = Net()\n","print(model)\n","\n","# Move tensors to GPU if CUDA is available\n","flag_cuda = torch.cuda.is_available()\n","if flag_cuda:\n","  model.cuda()\n","\n","# Specifying the loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Specify optimizer\n","optimizer = optim.Adam(model.parameters(), lr=.001)\n","#optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DFZ-InOHpCBg"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Specifying the number of epochs\n","n_epochs = 8\n","\n","def trainNet(model,criterion,optimizer,n_epochs,flag_cuda):\n","  # Unpacking the number of epochs to train the model\n","  epochs_list = [*range(1,n_epochs+1)]\n","\n","  # List to store loss to visualize\n","  train_losslist = []\n","  valid_losslist = []\n","  valid_loss_min = np.Inf # track change in validation loss\n","\n","  for epoch in epochs_list:\n","      # Keeping track of training and validation loss\n","      train_loss = 0.0\n","      valid_loss = 0.0\n","      \n","      ######################\n","      # Training the model #\n","      ######################\n","      model.train()\n","      for data, target in train_loader:\n","          # Moving tensors to GPU if CUDA is available\n","          if flag_cuda:\n","              data, target = data.cuda(), target.cuda()\n","          # Clearing the gradients of all optimized variables\n","          optimizer.zero_grad()\n","          # Forward pass: Computing predicted outputs\n","          output = model(data)\n","          # Calculating the batch loss\n","          loss = criterion(output, target)\n","          # Backward pass: compute gradient of loss with respect to parameters\n","          loss.backward()\n","          # Perform a single optimization step (parameter update)\n","          optimizer.step()\n","          # Update training loss\n","          train_loss += loss.item()*data.size(0)\n","          \n","      ########################    \n","      # Validating the model #\n","      ########################\n","      model.eval()\n","      for data, target in valid_loader:\n","          # Moving tensors to GPU if CUDA is available\n","          if flag_cuda:\n","              data, target = data.cuda(), target.cuda()\n","          output = model(data)\n","          loss = criterion(output, target)\n","          valid_loss += loss.item()*data.size(0)\n","      \n","      # Calculating average losses\n","      train_loss = train_loss/x_train_len\n","      valid_loss = valid_loss/x_val_len\n","      train_losslist.append(train_loss)\n","      valid_losslist.append(valid_loss)\n","          \n","      # Printing training/validation statistics \n","      print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","          epoch, train_loss, valid_loss))\n","      \n","      # Saving model if validation loss has decreased\n","      if valid_loss <= valid_loss_min:\n","          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","              valid_loss_min,valid_loss))\n","          torch.save(model.state_dict(), 'model_cifar.pt')\n","          valid_loss_min = valid_loss\n","        \n","  return epochs_list, train_losslist, valid_losslist\n","\n","# Executing the training\n","epochs_list, train_losslist, valid_losslist = trainNet(\n","    model,criterion,optimizer,n_epochs,flag_cuda)\n","\n","# Loading the best model\n","model.load_state_dict(torch.load('model_cifar.pt'))\n","\n","# Plotting the learning curves\n","plt.plot(epochs_list, train_losslist, epochs_list, valid_losslist)\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend(['Training','Validation'])\n","plt.title(\"Performance of Baseline Model\")\n","plt.show()"]},{"cell_type":"code","source":["def saveModel(model, filename):\n","  competition_path = \"/content/drive/Shareddrives/Neural Nets/Competition/Phase 1/Code/Models\" #path to data folder in drive\n","  model_file_path = os.path.join(competition_path, filename+\".pt\")  #joing path and adding .npy to filename passed in\n","  torch.save(model.state_dict(), model_file_path)\n","\n","saveModel(model, \"nathan_messing_around_recangle_kernel_2\")"],"metadata":{"id":"ObFWmUacdycP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78wEq6eOoxhM"},"outputs":[],"source":["\n","classes = [0, 1, 2, 3]\n","\n","def assessNet(model,criterion):\n","  # Tracking test loss and accuracy\n","  test_loss = 0.0\n","  class_correct = list(0. for i in range(len(classes)))\n","  class_total = list(0. for i in range(len(classes)))\n","\n","  # Setting model to evaluate\n","  model.eval()\n","\n","  # Iterating over batches of test data\n","  for data, target in test_loader:\n","      # Obtaining predictions and loss\n","      if flag_cuda:\n","          data, target = data.cuda(), target.cuda()\n","      output = model(data)\n","      loss = criterion(output, target)\n","      test_loss += loss.item()*data.size(0)\n","\n","      # Converting output probabilities to predicted class\n","      _, pred = torch.max(output, 1)    \n","      # Comparing predictions to true label\n","      correct_tensor = pred.eq(target.data.view_as(pred))\n","      correct = np.squeeze(correct_tensor.numpy()) if not flag_cuda else np.squeeze(correct_tensor.cpu().numpy())\n","      # Calculating test accuracy for each object class\n","      for i in range(batch_size):\n","          label = target.data[i]\n","          class_correct[label] += correct[i].item()\n","          class_total[label] += 1\n","\n","  # Computing the average test loss\n","  test_loss = test_loss/len(test_loader.dataset)\n","  print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","  # Computing the class accuracies\n","  for i in range(4):\n","      if class_total[i] > 0:\n","          print('Test Accuracy of %10s: %2d%% (%2d/%2d)' % (\n","              classes[i], 100 * class_correct[i] / class_total[i],\n","              np.sum(class_correct[i]), np.sum(class_total[i])))\n","      else:\n","          print('Test Accuracy of %10s: N/A (no training examples)' % (classes[i]))\n","\n","  # Computing the overall accuracy\n","  print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n","      100. * np.sum(class_correct) / np.sum(class_total),\n","      np.sum(class_correct), np.sum(class_total)))\n","  \n","  return\n","\n","# Executing the assessment\n","assessNet(model,criterion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNbnZxxQHCMp"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Messing Around Some More.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}